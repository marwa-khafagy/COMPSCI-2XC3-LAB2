% latex boilerplate 

\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{tikz-cd}
\usepackage{parskip}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage[section]{placeins}

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

\newcommand{\firgureBuffered}[2]
{
    \begin{figure}[ht!]
        \centering
        \includegraphics[width=1\textwidth]{#1}
        \caption{#2}
    \end{figure}
}

\newcommand{\firgureDualBuffered}[2]
{
    \begin{figure}[ht!]
        \begin{subfigure}[t]{.45\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{#1_1.png}
            \caption{First Run.}
        \end{subfigure}\hfill
        \begin{subfigure}[t]{.50\textwidth}
            \centering
            \includegraphics[width=1\textwidth]{#1_2.png}
            \caption{Second Run.}
        \end{subfigure}
    \caption{#2}
    \end{figure}
}

\newcommand{\figureSmaller}[2]
{
    \begin{figure}[ht!]
        \centering
        \includegraphics[width=1\textwidth,scale=0.5]{#1}
        \caption{#2}
    \end{figure}
}

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

% Hide Numbers on Sections
\setcounter{secnumdepth}{0}

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

\title{COMPSCI 2XC3 Lab 3}
\author{Marwa Khafagy, Om Patel, Alex Eckardt}
\date{February 27, 2023}

\begin{document}

\maketitle

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\newpage
\tableofcontents

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\newpage
\listoffigures


%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
%
%
%                       EXPERIMENT 1
%
%
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\newpage
\section{Part 1}
\subsection{Experiment 1}

\subsubsection{Graph Class Optimizations}

To squeeze some performance out of the Graph class, especially at larger scales, we do some optimizations.

Firstly, when adding an edge to a node, if we are adding a self-loop, then we only add that to the list once. This of course means that we assume we are working with self-loops.

The next improvement is memorizing the nodes that contain edges. When we add a node, we mark it in a dictionary as `marked'--IE, we store that this node has an edge on it.

This dictionary's keys can then be used to check for a cycle instead of the adjacency list's-- we don't spend time checking nodes which have no edges if they contribute to a cycle. This change significantly impacts performance, especially on test 8 of this Experiment.

\subsubsection{Test Outlines}

Experiment 1 conducts three types of tests in order to XXX\@. These tests include Proportionality Test, Proportionality Test*, and the Addition Test.

\subsubsection*{Proportionality Test}
The first test, Proportionality, works as follows:

\begin{itemize}
    \item We create a random graph $G$ with constant $V$ nodes and $E$ amount of edges.
    \item We check if $G$ contains a cycle. If so, add to a count.
    \item Repeat above $X$ times, and get the proportion of cycle vs non-cycle.
    \item Repeat this entire process for a range of $E$.
\end{itemize}

\subsubsection*{Proportionality Test*}
The next test, Proportionality Test* follows exactly from Proportionality Test, but does not allow for a definition of $E$. Instead, $E$ is calculated as

\begin{center}
    $E = \frac{V^2 + V}{2}$
\end{center}

This is the maximum number of edges that can be added to an undirected graph, with self loops. This way, we explore the range of all possible number of edge counts.

\subsubsection*{Addition Test}

This test works as follows.

\begin{itemize}
    \item We initialize graph $G$ with $V$ nodes and 0 edges.
    \item We add an edge to $G$.
    \item Repeat this process until $G$ contains a cycle or no more edges can be added to $G$.
    \item When a cycle is found, track how many edges were added to $G$.
    \item After this process is repeated after $X$ trials, we find the average number of edges added to create a cycle.
    \item We also keep track of the minimum and maximum number of edges were added.
    \item Repeat this entire process for a range of $V$.
\end{itemize}

\subsubsection{Tests}

The first test conducted in Experiment 1 is a Proportionality Test*, on 1000 trials on graphs with 5 nodes.

The second test is a Proportionality Test*, on 1000 trials on graphs with 10 nodes.

Next, a Proportionality Test*, on 1000 trials on graphs with 30 nodes.

Afterwards, we perform a regular Proportionality Test with 100 trials on graphs with 100 nodes. We define the max edge count to be 1000. Had we used the Proportionality* Test, we have a max edge count of 5050.

Next we perform our Addition tests.

First we do the test on ranges 0--5, averaging the result between 1000 trials.

Then, we do it on the range 0--100, again averaging the result on 1000 trials.

Penultimately, we perform the Addition Test on the range 0 to 1000, skipping every 10. Here, we only average based on 10 trials.

We then do we perform the Addition Test on the range 0 to 10000, skipping every 500. With 40 trials, to see if anything strange occurs on larger graph sizes.


\subsubsection{Proportionality Test Results and Conclusions}

\FloatBarrier{}
\firgureBuffered{images/experiment1/exp1_a.png}{Proportionality* Test on graph sizes 0--5, 0--15 Edges}
\firgureBuffered{images/experiment1/exp1_b.png}{Proportionality* Test on graph sizes 0--10, 0--55 Edges}
\firgureBuffered{images/experiment1/exp1_c.png}{Proportionality* Test on graph sizes 0--30, 0--465 Edges}
\firgureBuffered{images/experiment1/exp1_d.png}{Proportionality Test on graph sizes 0--100, 0--1000 Edges}
\FloatBarrier{}

What we can see from the proportionality tests is that eventually we see that the average number of cyclical graphs hits 1.0. This means that, in our trials, after some certain number of edge count, every single graph we created was cyclical.

We can see the ratio of edges we need in the random graph as follows:
\begin{itemize}
    \item Node Count of 5, we need 5 edges to always create a cycle.
    \item Node count of 10, we need 10 edges to always create a cycle.
    \item Node count of 30, we need 27 edges to always create a cycle.
    \item Node count of 100, we need 75 edges to always create a cycle.
\end{itemize}

What we can determine is that if we have a graph of $V$ nodes, if we randomly insert $V$ amount of edges, we guarantee a cycle to be created.

This makes sense if we consider a graph with $V$ nodes and $V$ edges. If we wanted to minimize the number of cycles in this graph, we would make every node connect only to it's neighbours. This way, each node is only ever connected to 2 nodes. If the cycle we create here contains exactly every node, there would only be one cycle in the graph.
Let us name this graph $G^{*}$, the graph with one cycle and $V$ edges.

\FloatBarrier{}
\firgureBuffered{images/experiment1/exp1_exp1.png}{Example Graphs $V$ nodes and $V$ edges, $G^{*}$}
\FloatBarrier{}

If we were to add a node at random to $G$, we see would would create another cycle. As every node is already connected to every other, we now have two paths to every other node.

Likewise, if we were to remove a node from $G^{*}$, we would be forced to break our cycle. This means that, if we have $V-1$ edges, we are not guaranteed to have a cycle.

What we can conclude is that if we have a graph with $V$ nodes and $V$ edges, we are guaranteed to have a cycle. The disparities that we find in tests 3 and 4 is that, it is astronomically more likely as $V$ increases that we create a cycle before we hit $V$ edges (Think self loop being very easy to create). Had we performed more trials, or create every possible graph, we would see that Tests 3 and 4 would also require $V$ edges to create a cycle, not some number less than $V$.

\subsubsection{Addition Test Results and Conclusions}

\FloatBarrier{}
\firgureBuffered{images/experiment1/exp1_e.png}{Addition Test on graph sizes 0--5}
\firgureBuffered{images/experiment1/exp1_f.png}{Addition Test on graph sizes 0--100}
\firgureBuffered{images/experiment1/exp1_g.png}{Addition Test on graph sizes 0--1000, skipping 10.}
% \firgureBuffered{images/experiment1/exp1_h.png}{Addition Test on graph sizes 0-10000, skipping 500.}
\FloatBarrier{}

What we can conclude here is very similar to the conclusions we made in the proportionality tests.

If we look at the minimum number of edges need to be added to create a cycle, we see that it stays constant at 1. This is very easy to explain, the edge we added connected some node to itself. This is ofcourse able because we assume we are working with graphs that can contain self loops.

If we look at the maximum number of edges needed to be added to create a cycle, we see that it tends to be close but also exactly at $V$. If we look at Test 6, we see that, for x <= 20, x = y; meaning the number of nodes is equal to the number of edges. When we go above, the number of edges added is once again below the node count, as in the 1000 trials, we never create a graph with a cycle as described by graph $G^{*}$ above. Again, had we created every single possible graph with $V$ nodes, we would have seen that this number sticks exactly to x=y.

Obviously, we see that the average number of edges need to create a cycle seems stay in between the maximum and minimum number of edges.

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
%
%
%                       EXPERIMENT 2
%
%
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\newpage
\subsection{Experiment 2}


\subsubsection{Test Outlines}

Experiment 2 performs 2 types of tests, which act almost identically to the tests in Experiment 1.

\subsubsection*{Proportionality* Test}
The style of test Proportionality*, works as follows:

We define $E'$ as a `constant', where $E'$ is defined as
\begin{center}
    $E' = \frac{V^2 + V}{2}$
\end{center}

Where $V$ is the number of nodes in the graph.

\begin{itemize}
    \item We create a random graph $G$ with constant $V$ nodes and $E$ amount of edges.
    \item We check if $G$ is connected. If so, add to a count.
    \item Repeat above $X$ times, and get the proportion of connected vs non-connected.
    \item Repeat this entire process for $E$ between the range of 0 to $E'$.
\end{itemize}


This is the maximum number of edges that can be added to an undirected graph, with self loops. This way, we explore the range of all possible number of edge counts.

This works best for small graphs, so that the edge count is not extraordinarily high. For larger graphs we use the unstarred Proportionality test.


\subsubsection{Tests}

As all the tests are proportionality tests, we define the inputs here.

The first test conducted in Experiment 2 is a Proportionality Test*, on 1000 trials on graphs with 5 nodes.

The second test is a Proportionality Test*, on 1000 trials on graphs with 10 nodes.

Finally, a Proportionality Test*, on 1000 trials on graphs with 30 nodes.

\subsubsection{Proportionality Test Results and Conclusions}

\FloatBarrier{}
\firgureBuffered{images/experiment2/a.png}{The Number of Random Edges vs Percentage of Connected 5 Node Graphs out of 1000}
\firgureBuffered{images/experiment2/b.png}{The Number of Random Edges vs Percentage of Connected 10 Node Graphs out of 1000}
\firgureBuffered{images/experiment2/c.png}{The Number of Random Edges vs Percentage of Connected 30 Node Graphs out of 1000}
\FloatBarrier{}

First, we can see that in all three graphs, the proportion of graphs that are connected stay at 0 until number of edges matches the number of nodes, minus 1.
This is especially noticeable in the graph of node counts of 5, where the first data point comes at $x=4$.

The reason this occurs is clear if we imagine a cyclical graph $G^{*}$, just as we did above in experiment 1. This graph is obviously connected. If we end up removing one edge from this cycle, the graph still remians connected.
This is the best case for connectivity, which explains why $x=4$ has a relativley small proportionality of connected graphs.


To come up with an equation, we think about how we can create a connected graph, or rather, at what edge case the graph would have to be in order for it to still be NOT connected.

Imagine a graph with every possible edge added to it. We have already stated that the number of edges in this graph would be
\begin{center}
    $E = \frac{V^2 + V}{2}$
\end{center}

Now we think how to force our graph to be disconnected. To do this, we pick some node and remove every edge connected to it, save for the edge that connects to itself (which does not impact connectivity). Since, in our graph, every node is connected to the other $V-1$ nodes, we can remove those $V-1$ connections.

This leaves us with our final graph, where every node is connected to every other node, save for one extra that is only connected to itself.

To force the graph to be connected, we just add one more edge coming from the isolated node to any other node, which must then, by our definition, connect the entire graph.

Thus, our equation to find the amount of edges to force a connected node is
\begin{center}
    $E = \frac{V^2 + V}{2} - (V-1) + 1$\\
    $E = \frac{V^2 - V}{2} + 2$
\end{center}

Looking at the edge count for the graph sizes we tested on, we get the following number of edges to force a connective graph

\begin{itemize}
    \item $E(5) = 12$ 
    \item $E(10) = 47$ 
    \item $E(30) = 437$ 
\end{itemize}

While we do get the correct number for a graph with 5 nodes, we do not get the correct number of edges for a graph of 10 or 30 nodes. This is most likely due to the fact that as we get higher and higher node counts, the
amount of `permutations' of edge placements grows rapidly. To get results where the proportion of graphs being connected is always 1, we must not be creating enough random graphs to affect the results.

This could be improved upon by performing many more trials, or by actually creating each `permutation' of the graph and checking how many are connected.

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
%
%
%                       EXPERIMENT 3
%
%
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\newpage
\section{Part 2}
\subsection{Approximation Experiments}

\subsubsection{Test Outlines}

The approximation experiments conducts three different experiments to test the effectiveness of the three approximation algorithms given on different types of graphs.

\subsubsection{Peformance by Number of Nodes Test}

The first experiement will test the performance of the three approximation algorithms by the number of nodes in the graph.

The first experiment is as follows: 

\begin{itemize}
    \item Create a graph with $V$ nodes and $E$ edges where $E = \frac{V^2 + V}{2}$. This is the maximum number of edges that can be added to an undirected graph, with self loops.
    \item Run the three approximation algorithms (as well as the Minimum Vertex Cover algorithm) on the graph 1000 times, taking the sum of the size of the VC set for each algorithm.
    \item Repeat this process for $V$ ranging from 1 to 24, in increments of 1. (A larger range would take too long to run.)
    \item Plot the ratio between the sum of the size of the VC set for each algorithm and the sum of the size of the VC set for the Minimum Vertex Cover algorithm.
\end{itemize}
\newpage
\subsubsection{Performance by Number of Nodes Results and Conclusions}

\FloatBarrier{}
\firgureBuffered{images/approxexperiments/Nodes.png}{Performance of Approximation by Number of Nodes}
\FloatBarrier{}

We can see from the graph above that was the number of nodes increases, the approximation algorithms get closer to the optimal solution. However, Approximation 1 is the most accurate in that it relatively consistently is able to compute the minimum vertex cover, save for a few spikes (which are still very close to the actual minimum vertex cover). This is expected since it is the only approximation algorithm that computes the MVC with a strategy, rather than randomness like the other 2. \\

Approximation 3 has a big jump as the beginning (much higher than Approximation 2), but decreaes as soon as the number of node increases. Approximation 2 and 3 consistently have an extremely similar ratio after about 7 nodes and get very close to the actual MVC after 20 nodes. Finding the ratio for more nodes is possible but would take a very long time to run since to find the actual MVC, the power set must be computed which takes $O(2^V)$ time. \\

\subsubsection{Performance by Number of Edges Test}

The second experiment will test the performance of the three approximation algorithms by the number of edges in the graph.

The second experiment is as follows:

\begin{itemize}
    \item Create a graph with $25$ nodes and $E$ edges.
    \item Run the three approximation algorithms (as well as the Minimum Vertex Cover algorithm) on the graph 1000 times, taking the sum of the size of the VC set for each algorithm.
    \item Repeat this process for $E$ ranging from 1 to 196, in increments of 5. (A larger range would take too long to run.)
    \item Plot the ratio between the sum of the size of the VC set for each algorithm and the sum of the size of the VC set for the Minimum Vertex Cover algorithm.
\end{itemize}
\newpage
\subsubsection{Performance by Number of Edges Results and Conclusions}

\FloatBarrier{}
\firgureBuffered{images/approxexperiments/Edges.png}{Performance of Approximation by Number of Edges}
\FloatBarrier{}

We can see from the graph above, that as the number of edges increases, the approximation algorithms get closer to the optimal solution, similarly to the first experiment. Approximation 1 is still the best approximation algorithm regardless of the number of edges as it is the only one that computes the MVC with a strategy.

Approximation 2 and 3 initially are very far off from the MVC, with approximation 2 and 3 having a ratio of about 3.1 and 2, respectively. Approximation 2 may take so much longer because it works by selecting nodes (which remain constant), rather than selecting edges like Approximation 3 does since the number of edges is the variable which changes. \\

However, as the number of edges increases, Approximation 2 and 3 get closer and closer to the actual MVC. Approximation 2 and 3 have a ratio of $<1.1$ after 190 edges. \\

\subsubsection{Performance by size of Tree Test}

The third experiment is different in that it will test the performance of the three approximation algorithms on tree graphs with a varying number of nodes. Formally in Graph Theory, a tree is a connected acyclic undirected graph. \\

The third experiment is as follows:

\begin{itemize}
    \item Create a tree with $V$ nodes and $E$ edges where $E = V-1$. This is the maximum number of edges that can be added to a tree.
    \item Run the three approximation algorithms (as well as the Minimum Vertex Cover algorithm) on the graph 1000 times, taking the sum of the size of the VC set for each algorithm.
    \item Repeat this process for $V$ ranging from 1 to 24, in increments of 1. (A larger range would take too long to run.)
    \item Plot the ratio between the sum of the size of the VC set for each algorithm and the sum of the size of the VC set for the Minimum Vertex Cover algorithm.
\end{itemize}
\newpage
\subsubsection{Performance by size of Tree Results and Conclusions}

\FloatBarrier{}
\firgureBuffered{images/approxexperiments/Tree1.png}{Performance of Approximation by Size of Tree}
\FloatBarrier{}

This experiment yields some extremely interesting results.

For Approximation 1, it consistently computes the minimum vertex cover for all tree graphs, save for a few spikes. These are the same results as the first and secondexperiment. \\

The ratio for Approximation 2 seems to be increasing as the number of nodes increases. This is extremely different from experiment 1, where the ratio was decreasing as the number of nodes increased. By extrapolating, we can see that if more nodes were to be added, the ratio would keep increasing. The tree property of these graphs seems to make the algorithm perform worse.\\

Approximation 3 appears to stay consistently around a ratio of 1.7 after 10 nodes, and before 10 nodes is consistently around a ratio of 1.9. If more nodes were to be added, the ratio would likely stay around 1.7. \\

\subsection{The Independent Set Problem}
In this experiment, we performed tests to empirically find the relationship between the maximum 
independent set (MIS) and the minimum vertex cover (MVC) of an undirected graph.

\subsubsection{Test Outlines}

In the first test we ran, we graphed the sum of the cardinalities of MIS and MVC.

This first test, works as follows:

\begin{itemize}
    \item We create a random graph $G$ with $n$ nodes and some randomly generated edges $E$
    \item We find the cardinality of MIS
    \item We find the cardinality of MVC
    \item We sum these two cardinalities 
    \item Repeat this process for $x$ amount of trials (we chose 5 trials)
    \item We average the sum of these two cardinalities and plot it 
    \item Repeat this entire process for a range of graph sizes (we chose to go up to graphs with 25 nodes)
\end{itemize}

In the second test we ran, we graphed cardinalities of MIS, MVC and the complement of MVC.

This second test, works as follows:

\begin{itemize}
    \item We create a random graph $G$ with $n$ nodes and some randomly generated edges $E$
    \item We find the cardinality of MIS
    \item We find the cardinality of MVC
    \item We check if the complement of MVC is an independent set, raising an exception if it is not
    \item Repeat this process for $x$ amount of trials (we chose 5 trials)
    \item We plot the averages of cardinality of MIS, MVC, and we also plot the cardinality of the complement of MVC
    \item Repeat this entire process for a range of graph sizes (we chose to go up to graphs with 25 nodes)
\end{itemize}

In the third test we ran, we graphed cardinalities of MIS, MVC and the complement of MIS.

This second test, works as follows:

\begin{itemize}
    \item We create a random graph $G$ with $n$ nodes and some randomly generated edges $E$
    \item We find the cardinality of MIS
    \item We find the cardinality of MVC
    \item We check if the complement of MIS is a vertex cover, raising an exception if it is not
    \item Repeat this process for $x$ amount of trials (we chose 5 trials)
    \item We plot the averages of cardinality of MIS, MVC, and we also plot the cardinality of the complement of MIS
    \item Repeat this entire process for a range of graph sizes (we chose to go up to graphs with 25 nodes)
\end{itemize}

\newpage
\subsubsection{Test Results}

\FloatBarrier{}
\firgureBuffered{images/IndSet/Figure_1.png}{Test 1}
\firgureBuffered{images/IndSet/Figure_2.png}{Test 2}
\firgureBuffered{images/IndSet/Figure_3.png}{Test 3}
\FloatBarrier{}

\subsubsection{Conclusions}
Upon performing the first test, we found that the sum of the cardinalities of MIS and MVC are always equal
to the number of nodes in the graph. 

Through the second test we found that the complement of MVC has the same cardinality of a MIS, and is an independent set.

Through the third test we found that the complement of MIS has the same cardinality of a MVC, and is a vertex cover.

In conclusion, these three tests show empirically that the complement of a MVC is a MIS, and vise versa. Thus, given a MIS, 
one can find a MIS (and vise versa). We also note that there may exists more than one MIS/MVC for a given graph, so checking 
to see if the complement of MVC is equal to a given MIS may return false. 



%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
%
%
%                       APPENDIX
%
%
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\newpage
\section{Appendix}

Experiment 1's tests are conducted in experiment1.py.
The function for each test is as follows.
\begin{itemize}
    \item Proportionality Test = proportionality\_test\(\)
    \item Proportionality Test* = max\_proportionality\_test\(\)
    \item Addition Test = edge\_additions\_until\_cycle\_test\(\)
\end{itemize}
$ $
\newline
All the actual tests performed are outlined at the bottom on experiment1.py.
\newline
$ $
\newline

Experiment 2's tests are conducted in experiment2.py.
The function for each test is as follows.
\begin{itemize}
    \item Proportionality Test = proportionality\_test\(\)
    \item Proportionality Test* = max\_proportionality\_test\(\)
\end{itemize}
$ $
\newline
All the actual tests performed are outlined at the bottom on experiment2.py.
\\
\\

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
% Approximation Experiments appendix
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

The Approximation algorithms can be found in graph.py. \\
The Approximation 1, 2, and 3's experiments are conducted in approxexp1.py.
\\
\begin{itemize}
    \item Performance by Number of Nodes = approximation\_performance\_by\_edges\_tests\(\)
    \item Performance by Number of Edges = approximation\_performance\_by\_edges\_tests\(\)
    \item Performance by Size of Tree = approximation\_performance\_by\_trees\(\)
\end{itemize}

%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
% Independent Set Problem appendix
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-


Independent Set tests are conducted in IndependentSet.py.
The function for each test is as follows.
\begin{itemize}
    \item Sum of MIS and MVC Test = size\_of\_MIS\_MVC\_test\(\)
    \item Complement of MVC Test* = len\_comp\_MVC\(\)
    \item Complement of MIS Test* = len\_comp\_MIS\(\)
\end{itemize}
$ $
\newline
All the actual tests performed are outlined at the bottom on IndependentSet.py.


%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
%=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-

\newpage
\section{Executive Summary}

\subsubsection{Experiment 1}
\begin{itemize}
    \item The smallest number of edges needed to be added to a graph to create a cycle is 1 (a self loop)
    \item It would require at most $V$ number of edges added to force a cycle to be created in a graph (worst case) 
  \end{itemize}

\subsubsection{Experiment 2}
\begin{itemize}
    \item The smallest number of edges needed to force a cycle to be connected would be obviously $V-1$ (best case, where we create one large cycle, save for one edge).
    \item It would require at most $\frac{V^2 - V}{2} + 2$ number of edges added to force a cycle to be connected in a graph (worst case) 
  \end{itemize}

  \subsubsection{Approximation Experiment by Number of Nodes}

  \begin{itemize}
      \item Approximation 1 performs the best out of the three approximation algorithms. It consistently computes the minimum vertex cover for all graphs with a varying number of nodes.
      \item Approximation 1 may perform the best because it computes the minimum vertex cover with a strategy, rather than randomness like the other two approximation algorithms.
      \item Approximation 2 and 3 consistently have an extremely similar ratio after about 7 nodes and get very close to the actual MVC after 20 nodes.
  \end{itemize}
  
  \subsubsection{Approximation Experiment by Number of Edges}
  
  \begin{itemize}
      \item Approximation 1 is still the best approximation algorithm regardless of the number of edges as it is the only one that computes the MVC with a strategy.
      \item Approximation 2 and 3 initially are very far off from the MVC, with approximation 2 and 3 having a ratio of about 3.1 and 2, respectively.
      \item Approximation 2 and 3 have a ratio of $<1.1$ after 190 edges.
      \item All approximation algorithms have a similar ratio after 190 edges.
  \end{itemize}
  
  \subsubsection{Approximation Experiment by Size of Tree}
  
  \begin{itemize}
      \item Approximation 1 consistently computes the minimum vertex cover for all tree graphs, save for a few spikes.
      \item The ratio for Approximation 2 seems to be increasing as the number of nodes increases. This is extremely different from experiment 1, where the ratio was decreasing as the number of nodes increased.
      \item Approximation 3 appears to stay consistently around a ratio of 1.7 after 10 nodes, and before 10 nodes is consistently around a ratio of 1.9.
  \end{itemize}
  

\subsubsection{Independent Set Problem}
  \begin{itemize}
    \item The sum of the cardinalities of MIS and MVC result in the number of nodes in a given graph
    \item Thus, given the cardinality of MIS, one can easily find the cardinality of MVC (and vise versa)
    \item The complement of an MIS is an MVC (and vise versa)
  \end{itemize}

\end{document}